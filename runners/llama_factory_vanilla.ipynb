{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/finetune_forge/runners\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/finetune_forge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Change directory (go back to the root of the repository).\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/finetune_forge\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGING_FACE_FINE_TUNING_WRITE_ACCESS_TOKEN = os.getenv(\"HUGGING_FACE_FINE_TUNING_WRITE_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/marx/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace.\n",
    "!huggingface-cli login --token $HUGGING_FACE_FINE_TUNING_WRITE_ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "05/17/2024 00:49:25 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "05/17/2024 00:49:25 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:49:26,172 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:49:26,173 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:49:26,173 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:49:26,173 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:49:26,173 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 00:49:26 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "05/17/2024 00:49:26 - INFO - llmtuner.data.loader - Loading dataset MediaMeter/Chapterization...\n",
      "Converting format of dataset: 100%|█| 743/743 [00:00<00:00, 10472.23 examples/s]\n",
      "Running tokenizer on dataset: 100%|███| 743/743 [00:05<00:00, 126.75 examples/s]\n",
      "input_ids:\n",
      "[1, 28705, 733, 16289, 28793, 995, 460, 3638, 286, 395, 9131, 288, 8598, 1871, 442, 19451, 477, 272, 2296, 1945, 28733, 1431, 12690, 28723, 12628, 264, 5511, 302, 2245, 28725, 10661, 653, 304, 8270, 264, 9292, 5032, 390, 3825, 28723, 9822, 272, 11382, 3624, 298, 1316, 368, 297, 20365, 272, 3825, 28747, 13, 13, 28740, 28723, 619, 16778, 1793, 272, 7388, 28733, 1462, 3838, 8187, 4049, 13, 259, 387, 330, 1945, 28733, 1431, 5964, 12335, 302, 28747, 13, 355, 387, 1552, 28742, 3499, 28742, 28832, 28747, 330, 1830, 28733, 4404, 23094, 442, 5436, 302, 272, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 9899, 28742, 28832, 28747, 330, 1274, 302, 12489, 970, 264, 7735, 349, 264, 1407, 302, 2245, 369, 4180, 1377, 264, 633, 4211, 442, 10661, 297, 264, 3248, 28723, 661, 349, 9566, 1221, 11985, 9494, 346, 477, 272, 2191, 2187, 2245, 298, 1876, 575, 28725, 2608, 1250, 287, 3702, 28725, 297, 264, 6084, 6205, 28725, 442, 16841, 23138, 28723, 9655, 404, 460, 3078, 864, 28725, 18062, 3864, 272, 3036, 369, 6104, 28725, 304, 590, 8327, 272, 8847, 1059, 272, 3248, 28742, 28713, 4693, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1666, 9899, 28742, 28832, 28747, 330, 1274, 302, 1083, 9899, 970, 1083, 9899, 460, 3684, 298, 12489, 562, 4312, 13097, 1083, 24415, 2373, 264, 6084, 4211, 28723, 1306, 460, 2608, 1221, 11985, 298, 347, 7191, 2108, 15574, 821, 2191, 12489, 28725, 562, 1309, 9494, 477, 272, 2187, 2245, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 18561, 28730, 26644, 28742, 28832, 28747, 330, 1274, 302, 19443, 302, 5160, 19810, 10248, 297, 272, 2245, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 405, 1416, 804, 28713, 28742, 28832, 28747, 330, 1274, 302, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 2245, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1158, 2045, 28742, 28832, 28747, 415, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 13, 28750, 28723, 619, 24958, 1298, 7122, 9148, 4049, 13, 259, 387, 24408, 356, 9131, 288, 272, 1080, 8598, 1871, 442, 19451, 477, 1430, 11077, 28723, 851, 829, 3024, 1945, 18978, 28725, 7974, 16582, 28725, 442, 5864, 4162, 28723, 13, 13, 28770, 28723, 619, 4712, 674, 418, 3000, 20465, 3523, 3159, 685, 4049, 13, 259, 387, 1529, 2107, 5160, 19810, 477, 264, 2078, 2245, 304, 25425, 272, 2903, 28725, 7501, 272, 3551, 354, 3235, 25308, 354, 1430, 9040, 28723, 28705, 13, 259, 387, 1136, 744, 302, 272, 1871, 9237, 1774, 28725, 9051, 304, 20577, 653, 707, 5160, 19810, 28725, 2490, 562, 459, 6516, 298, 22015, 4780, 28725, 3994, 28777, 1251, 12889, 4866, 28725, 6807, 28743, 4866, 28725, 4345, 2063, 2373, 272, 2245, 28723, 13, 259, 387, 415, 1587, 1023, 2169, 272, 25081, 19810, 297, 264, 28429, 5032, 28725, 9836, 354, 10537, 25308, 442, 4870, 1871, 354, 1430, 10248, 9040, 28723, 259, 13, 259, 387, 9822, 4668, 272, 2757, 3624, 354, 272, 5160, 19810, 28747, 13, 13, 355, 5160, 28730, 26644, 327, 733, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28740, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28740, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28708, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28750, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28726, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 3235, 25308, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28750, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28770, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28781, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 264, 3298, 5436, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 422, 3301, 680, 11507, 390, 3236, 13, 355, 4709, 13, 2287, 13, 259, 523, 4450, 28730, 28740, 28767, 304, 523, 4450, 28730, 28750, 28767, 2904, 1581, 4514, 302, 5160, 19810, 28723, 13, 259, 7066, 5846, 5876, 264, 1274, 302, 281, 3033, 4838, 28725, 970, 1430, 19443, 10651, 396, 9040, 304, 871, 5363, 5436, 28723, 13, 259, 7066, 9040, 541, 506, 396, 3235, 5436, 28723, 13, 13, 13, 28781, 28723, 619, 5155, 21824, 272, 17104, 2045, 4049, 13, 259, 387, 3348, 409, 272, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 560, 12856, 456, 15081, 916, 272, 1945, 1552, 28742, 1158, 2045, 28742, 9429, 13, 13, 28782, 28723, 619, 12822, 3646, 272, 4702, 1416, 804, 28713, 4049, 13, 259, 387, 15220, 1575, 304, 1274, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 11077, 916, 272, 1945, 1552, 28742, 405, 1416, 804, 28713, 28742, 9429, 13, 13, 28784, 28723, 619, 5404, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 298, 12489, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 6412, 354, 707, 3941, 1222, 442, 544, 7280, 2210, 28723, 13, 355, 387, 17361, 28747, 11772, 513, 707, 2245, 349, 438, 272, 5398, 302, 264, 4211, 442, 10969, 4411, 28723, 13, 355, 387, 13158, 28747, 3348, 409, 513, 272, 2245, 349, 3078, 864, 304, 9045, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6096, 707, 1474, 288, 442, 2841, 16212, 28723, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 707, 2245, 4180, 1377, 264, 633, 9067, 442, 4211, 28723, 13, 13, 28787, 28723, 619, 3540, 4983, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 302, 1083, 9899, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 5078, 9899, 2608, 1372, 3941, 1222, 28725, 562, 590, 1659, 459, 1743, 347, 297, 544, 7280, 2210, 28725, 15343, 741, 2191, 12489, 28723, 13, 355, 387, 17361, 28747, 4914, 513, 272, 2245, 8045, 2373, 264, 4211, 28725, 4312, 2296, 264, 2191, 7735, 28723, 5078, 9899, 460, 2608, 1307, 298, 13097, 1083, 3746, 1063, 2373, 264, 6084, 4211, 28723, 13, 355, 387, 13158, 28747, 5158, 21824, 513, 272, 2245, 349, 680, 2948, 821, 264, 2191, 7735, 562, 1309, 3078, 864, 28723, 661, 1023, 24340, 356, 272, 3036, 8160, 486, 272, 2191, 7735, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6412, 354, 13461, 442, 23793, 1474, 288, 442, 17144, 3569, 325, 28706, 28723, 28721, 2063, 345, 28740, 28723, 28740, 5078, 3007, 548, 345, 28899, 5078, 2275, 4145, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 272, 2245, 1792, 1303, 442, 8484, 28713, 1060, 272, 9067, 8160, 486, 272, 2191, 7735, 28725, 3210, 821, 25618, 264, 4716, 633, 9067, 28723, 13, 13, 28783, 28723, 619, 23342, 320, 1787, 348, 13, 259, 387, 5919, 8270, 12944, 2818, 356, 272, 2078, 8598, 1871, 304, 19470, 17154, 426, 477, 20365, 4606, 12944, 442, 4606, 2293, 28723, 13, 13, 28774, 28723, 619, 18325, 547, 16676, 16329, 4049, 13, 259, 387, 25421, 2169, 272, 1871, 25081, 477, 272, 5511, 302, 2245, 28723, 25217, 369, 272, 4162, 460, 3078, 864, 304, 5227, 1197, 28723, 13, 13, 28740, 28734, 28723, 619, 3490, 848, 9292, 12107, 4049, 13, 259, 387, 3838, 848, 346, 5032, 574, 2899, 390, 264, 9292, 1928, 28723, 7066, 1945, 28733, 1431, 5964, 1023, 616, 7750, 298, 272, 6140, 4693, 28723, 13, 13, 273, 16693, 7388, 28733, 1462, 367, 992, 28747, 13, 17422, 371, 13, 1417, 345, 3499, 1264, 345, 1313, 1949, 3626, 4628, 8429, 304, 3437, 14278, 302, 3332, 28725, 2490, 5122, 288, 10616, 4788, 23411, 354, 23427, 13519, 28713, 28725, 8050, 7161, 524, 28777, 10143, 263, 16585, 1413, 1008, 28733, 8554, 23313, 5168, 28725, 27698, 3842, 4994, 395, 4788, 23411, 28725, 21400, 5246, 5168, 9804, 395, 7062, 727, 28733, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382]\n",
      "inputs:\n",
      "<s>  [INST] You are tasked with extracting relevant information or identification from the following key-value pairs. Given a piece of text, chapterize and generate a JSON format as output. Follow the instructions below to help you in generating the output:\n",
      "\n",
      "1. **Understand the Key-Value Structure:**\n",
      "   - A key-value pair consists of:\n",
      "      - `'summary'`: A top-level overview or description of the chunk. Must not be empty (i.e., required).\n",
      "      - `'headers'`: A list of headers where a header is a line of text that introduces a new section or chapter in a document. It is typically formatted distinctly from the main body text to stand out, often being bolder, in a larger font, or differently styled. Headers are concise, summarizing the content that follows, and they guide the reader through the document's structure. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'subheaders'`: A list of subheaders where subheaders are similar to headers but usually introduce subsections within a larger section. They are often formatted to be slightly less prominent than main headers, but still distinct from the body text. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'named_entities'`: A list of dictionary of named entities identified in the text. Must not be empty (i.e., required).\n",
      "      - `'keypoints'`: A list of the most important elements or essential information conveyed in the given text chunk. Must not be empty (i.e., required).\n",
      "      - `'tonality'`: The overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Must not be empty (i.e., required).\n",
      "\n",
      "2. **Extract Relevant Information:**\n",
      "   - Focus on extracting the most relevant information or identification from each chunk. This could include key themes, essential concepts, or significant details.\n",
      "\n",
      "3. **Perform Named Entity Recognition:**\n",
      "   - Extract named entities from a given text and organize the results, providing the option for individual descriptions for each entity. \n",
      "   - As part of the information extraction, identify and categorize any named entities, including but not limited to PERSON, ORGANIZATION, LOCATION, etc., within the text.\n",
      "   - The system should present the extracted entities in a structured format, allowing for detailed descriptions or additional information for each identified entity.  \n",
      "   - Follow exactly the example below for the named entities:\n",
      "\n",
      "      named_entities = [\n",
      "         {\n",
      "            \"<Tag_1>\": [\n",
      "                  {\"entity\": \"<Entity_1>\", \"description\": \"<Description_1a>\"},\n",
      "                  {\"entity\": \"<Entity_2>\", \"description\": \"<Description_1b>\"},\n",
      "                  # Add more entities with individual descriptions as needed\n",
      "            ]\n",
      "         },\n",
      "         {\n",
      "            \"<Tag_2>\": [\n",
      "                  {\"entity\": \"<Entity_3>\", \"description\": \"<Description_2>\"},\n",
      "                  {\"entity\": \"<Entity_4>\", \"description\": \"<Description_2>\"},\n",
      "                  # Add more entities with a common description as needed\n",
      "            ]\n",
      "         },\n",
      "         # Add more entries as needed\n",
      "      ]\n",
      "   \n",
      "   <Tag_1> and <Tag_2> represent different types of named entities.\n",
      "   Each tag contains a list of dictionaries, where each dictionary represents an entity and its associated description.\n",
      "   Each entity can have an individual description.\n",
      "\n",
      "\n",
      "4. **Determine the Tonality:**\n",
      "   - Assess the overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Include this assessment under the key `'tonality'`.\n",
      "\n",
      "5. **Highlight the Keypoints:**\n",
      "   - Identify and list the most important elements or essential information conveyed in the given chunk under the key `'keypoints'`.\n",
      "\n",
      "6. **Header Detection:**\n",
      "   - Analyze the text for characteristics typical to headers of a document:\n",
      "      - Capitalization: Look for any title case or all uppercase.\n",
      "      - Position: Consider if any text is at the beginning of a section or stands alone.\n",
      "      - Content: Assess if the text is concise and focused.\n",
      "      - Numbering/Symbols: Note any numbering or special symbols.\n",
      "      - Contextual Relevance: Evaluate if any text introduces a new topic or section.\n",
      "\n",
      "7. **Subheader Detection:**\n",
      "   - Analyze the text for characteristics typical of subheaders of a document:\n",
      "      - Capitalization: Subheaders often follow title case, but they might not always be in all uppercase, unlike some main headers.\n",
      "      - Position: Check if the text appears within a section, usually following a main header. Subheaders are often used to introduce subtopics within a larger section.\n",
      "      - Content: Determine if the text is more specific than a main header but still concise. It should elaborate on the content introduced by the main header.\n",
      "      - Numbering/Symbols: Look for secondary or nested numbering or bullet points (e.g., \"1.1 Subsection\", \"• Subpoint\").\n",
      "      - Contextual Relevance: Evaluate if the text refines or narrows down the topic introduced by the main header, rather than introducing a completely new topic.\n",
      "\n",
      "8. **Generate Tags**\n",
      "   - Please generate tags based on the given relevant information and strictly refrain from generating empty tags or empty array.\n",
      "\n",
      "9. **Provide Clear Details:**\n",
      "   - Clearly present the information extracted from the piece of text. Ensure that the details are concise and informative.\n",
      "\n",
      "10. **Strict JSON Response:**\n",
      "   - Strictly format your response as a JSON object. Each key-value pair should adhere to the specified structure.\n",
      "\n",
      "         Example Key-Value Pair:\n",
      "            {\n",
      "               \"summary\": \"It explores potential applications and future directions of research, including constructing dynamic knowledge graphs for specialized verticals, enhancing KGTransformer capabilities using self-supervised learning, combining language models with knowledge graphs, comparing graph learning techniques with traditional time- {\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with H\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382]\n",
      "labels:\n",
      "{\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with H\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 00:49:39,810 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 00:49:39,811 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 00:49:39 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 00:49:39 - INFO - llmtuner.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 00:49:39,830 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 00:49:39,831 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:49:39,831 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 00:49:42,632 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 00:49:42,632 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 00:49:42,890 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:49:42,890 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 00:49:43 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "05/17/2024 00:49:43 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 00:49:43 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 00:49:43 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:626] 2024-05-17 00:49:43,169 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2048] 2024-05-17 00:49:43,284 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-05-17 00:49:43,284 >>   Num examples = 743\n",
      "[INFO|trainer.py:2050] 2024-05-17 00:49:43,284 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2051] 2024-05-17 00:49:43,284 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2054] 2024-05-17 00:49:43,284 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2055] 2024-05-17 00:49:43,284 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2056] 2024-05-17 00:49:43,284 >>   Total optimization steps = 11\n",
      "[INFO|trainer.py:2057] 2024-05-17 00:49:43,285 >>   Number of trainable parameters = 3,407,872\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][INFO|trainer.py:2316] 2024-05-17 00:50:11,991 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 28.7053, 'train_samples_per_second': 25.884, 'train_steps_per_second': 0.383, 'train_loss': 1379.5863389968872, 'epoch': 0}\n",
      "  0%|                                                    | 0/11 [00:28<?, ?it/s]\n",
      "[INFO|trainer.py:3305] 2024-05-17 00:50:11,992 >> Saving model checkpoint to saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 00:50:12,629 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 00:50:12,629 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-05-17 00:50:12,648 >> tokenizer config file saved in saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-05-17 00:50:12,648 >> Special tokens file saved in saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =          0\n",
      "  total_flos               =  3053026GF\n",
      "  train_loss               =  1379.5863\n",
      "  train_runtime            = 0:00:28.70\n",
      "  train_samples_per_second =     25.884\n",
      "  train_steps_per_second   =      0.383\n",
      "[INFO|modelcard.py:450] 2024-05-17 00:50:12,665 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "05/17/2024 00:50:40 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "05/17/2024 00:50:40 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:50:40,724 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:50:40,724 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:50:40,725 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:50:40,725 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:50:40,725 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 00:50:40 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "05/17/2024 00:50:40 - INFO - llmtuner.data.loader - Loading dataset MediaMeter/Chapterization...\n",
      "Converting format of dataset: 100%|█| 743/743 [00:00<00:00, 10579.77 examples/s]\n",
      "Running tokenizer on dataset: 100%|███| 743/743 [00:05<00:00, 128.21 examples/s]\n",
      "input_ids:\n",
      "[1, 28705, 733, 16289, 28793, 995, 460, 3638, 286, 395, 9131, 288, 8598, 1871, 442, 19451, 477, 272, 2296, 1945, 28733, 1431, 12690, 28723, 12628, 264, 5511, 302, 2245, 28725, 10661, 653, 304, 8270, 264, 9292, 5032, 390, 3825, 28723, 9822, 272, 11382, 3624, 298, 1316, 368, 297, 20365, 272, 3825, 28747, 13, 13, 28740, 28723, 619, 16778, 1793, 272, 7388, 28733, 1462, 3838, 8187, 4049, 13, 259, 387, 330, 1945, 28733, 1431, 5964, 12335, 302, 28747, 13, 355, 387, 1552, 28742, 3499, 28742, 28832, 28747, 330, 1830, 28733, 4404, 23094, 442, 5436, 302, 272, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 9899, 28742, 28832, 28747, 330, 1274, 302, 12489, 970, 264, 7735, 349, 264, 1407, 302, 2245, 369, 4180, 1377, 264, 633, 4211, 442, 10661, 297, 264, 3248, 28723, 661, 349, 9566, 1221, 11985, 9494, 346, 477, 272, 2191, 2187, 2245, 298, 1876, 575, 28725, 2608, 1250, 287, 3702, 28725, 297, 264, 6084, 6205, 28725, 442, 16841, 23138, 28723, 9655, 404, 460, 3078, 864, 28725, 18062, 3864, 272, 3036, 369, 6104, 28725, 304, 590, 8327, 272, 8847, 1059, 272, 3248, 28742, 28713, 4693, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1666, 9899, 28742, 28832, 28747, 330, 1274, 302, 1083, 9899, 970, 1083, 9899, 460, 3684, 298, 12489, 562, 4312, 13097, 1083, 24415, 2373, 264, 6084, 4211, 28723, 1306, 460, 2608, 1221, 11985, 298, 347, 7191, 2108, 15574, 821, 2191, 12489, 28725, 562, 1309, 9494, 477, 272, 2187, 2245, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 18561, 28730, 26644, 28742, 28832, 28747, 330, 1274, 302, 19443, 302, 5160, 19810, 10248, 297, 272, 2245, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 405, 1416, 804, 28713, 28742, 28832, 28747, 330, 1274, 302, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 2245, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1158, 2045, 28742, 28832, 28747, 415, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 13, 28750, 28723, 619, 24958, 1298, 7122, 9148, 4049, 13, 259, 387, 24408, 356, 9131, 288, 272, 1080, 8598, 1871, 442, 19451, 477, 1430, 11077, 28723, 851, 829, 3024, 1945, 18978, 28725, 7974, 16582, 28725, 442, 5864, 4162, 28723, 13, 13, 28770, 28723, 619, 4712, 674, 418, 3000, 20465, 3523, 3159, 685, 4049, 13, 259, 387, 1529, 2107, 5160, 19810, 477, 264, 2078, 2245, 304, 25425, 272, 2903, 28725, 7501, 272, 3551, 354, 3235, 25308, 354, 1430, 9040, 28723, 28705, 13, 259, 387, 1136, 744, 302, 272, 1871, 9237, 1774, 28725, 9051, 304, 20577, 653, 707, 5160, 19810, 28725, 2490, 562, 459, 6516, 298, 22015, 4780, 28725, 3994, 28777, 1251, 12889, 4866, 28725, 6807, 28743, 4866, 28725, 4345, 2063, 2373, 272, 2245, 28723, 13, 259, 387, 415, 1587, 1023, 2169, 272, 25081, 19810, 297, 264, 28429, 5032, 28725, 9836, 354, 10537, 25308, 442, 4870, 1871, 354, 1430, 10248, 9040, 28723, 259, 13, 259, 387, 9822, 4668, 272, 2757, 3624, 354, 272, 5160, 19810, 28747, 13, 13, 355, 5160, 28730, 26644, 327, 733, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28740, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28740, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28708, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28750, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28726, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 3235, 25308, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28750, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28770, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28781, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 264, 3298, 5436, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 422, 3301, 680, 11507, 390, 3236, 13, 355, 4709, 13, 2287, 13, 259, 523, 4450, 28730, 28740, 28767, 304, 523, 4450, 28730, 28750, 28767, 2904, 1581, 4514, 302, 5160, 19810, 28723, 13, 259, 7066, 5846, 5876, 264, 1274, 302, 281, 3033, 4838, 28725, 970, 1430, 19443, 10651, 396, 9040, 304, 871, 5363, 5436, 28723, 13, 259, 7066, 9040, 541, 506, 396, 3235, 5436, 28723, 13, 13, 13, 28781, 28723, 619, 5155, 21824, 272, 17104, 2045, 4049, 13, 259, 387, 3348, 409, 272, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 560, 12856, 456, 15081, 916, 272, 1945, 1552, 28742, 1158, 2045, 28742, 9429, 13, 13, 28782, 28723, 619, 12822, 3646, 272, 4702, 1416, 804, 28713, 4049, 13, 259, 387, 15220, 1575, 304, 1274, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 11077, 916, 272, 1945, 1552, 28742, 405, 1416, 804, 28713, 28742, 9429, 13, 13, 28784, 28723, 619, 5404, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 298, 12489, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 6412, 354, 707, 3941, 1222, 442, 544, 7280, 2210, 28723, 13, 355, 387, 17361, 28747, 11772, 513, 707, 2245, 349, 438, 272, 5398, 302, 264, 4211, 442, 10969, 4411, 28723, 13, 355, 387, 13158, 28747, 3348, 409, 513, 272, 2245, 349, 3078, 864, 304, 9045, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6096, 707, 1474, 288, 442, 2841, 16212, 28723, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 707, 2245, 4180, 1377, 264, 633, 9067, 442, 4211, 28723, 13, 13, 28787, 28723, 619, 3540, 4983, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 302, 1083, 9899, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 5078, 9899, 2608, 1372, 3941, 1222, 28725, 562, 590, 1659, 459, 1743, 347, 297, 544, 7280, 2210, 28725, 15343, 741, 2191, 12489, 28723, 13, 355, 387, 17361, 28747, 4914, 513, 272, 2245, 8045, 2373, 264, 4211, 28725, 4312, 2296, 264, 2191, 7735, 28723, 5078, 9899, 460, 2608, 1307, 298, 13097, 1083, 3746, 1063, 2373, 264, 6084, 4211, 28723, 13, 355, 387, 13158, 28747, 5158, 21824, 513, 272, 2245, 349, 680, 2948, 821, 264, 2191, 7735, 562, 1309, 3078, 864, 28723, 661, 1023, 24340, 356, 272, 3036, 8160, 486, 272, 2191, 7735, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6412, 354, 13461, 442, 23793, 1474, 288, 442, 17144, 3569, 325, 28706, 28723, 28721, 2063, 345, 28740, 28723, 28740, 5078, 3007, 548, 345, 28899, 5078, 2275, 4145, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 272, 2245, 1792, 1303, 442, 8484, 28713, 1060, 272, 9067, 8160, 486, 272, 2191, 7735, 28725, 3210, 821, 25618, 264, 4716, 633, 9067, 28723, 13, 13, 28783, 28723, 619, 23342, 320, 1787, 348, 13, 259, 387, 5919, 8270, 12944, 2818, 356, 272, 2078, 8598, 1871, 304, 19470, 17154, 426, 477, 20365, 4606, 12944, 442, 4606, 2293, 28723, 13, 13, 28774, 28723, 619, 18325, 547, 16676, 16329, 4049, 13, 259, 387, 25421, 2169, 272, 1871, 25081, 477, 272, 5511, 302, 2245, 28723, 25217, 369, 272, 4162, 460, 3078, 864, 304, 5227, 1197, 28723, 13, 13, 28740, 28734, 28723, 619, 3490, 848, 9292, 12107, 4049, 13, 259, 387, 3838, 848, 346, 5032, 574, 2899, 390, 264, 9292, 1928, 28723, 7066, 1945, 28733, 1431, 5964, 1023, 616, 7750, 298, 272, 6140, 4693, 28723, 13, 13, 273, 16693, 7388, 28733, 1462, 367, 992, 28747, 13, 17422, 371, 13, 1417, 345, 3499, 1264, 345, 1313, 1949, 3626, 4628, 8429, 304, 3437, 14278, 302, 3332, 28725, 2490, 5122, 288, 10616, 4788, 23411, 354, 23427, 13519, 28713, 28725, 8050, 7161, 524, 28777, 10143, 263, 16585, 1413, 1008, 28733, 8554, 23313, 5168, 28725, 27698, 3842, 4994, 395, 4788, 23411, 28725, 21400, 5246, 5168, 9804, 395, 7062, 727, 28733, 17384, 4994, 28725, 304, 7821, 1890, 19664, 298, 524, 28777, 10143, 263, 9191, 13, 1417, 345, 9899, 1264, 7367, 27230, 548, 345, 3942, 2161, 548, 345, 3278, 18637, 8883, 13, 1417, 345, 1666, 9899, 1264, 7367, 27230, 298, 272, 18463, 548, 345, 11278, 304, 14268, 548, 345, 1146, 3625, 10011, 2161, 548, 345, 2012, 12909, 302, 17412, 548, 345, 17147, 7982, 19370, 1308, 8883, 13, 1417, 345, 18561, 28730, 26644, 1264, 28705, 13, 359, 28705, 733, 13, 359, 260, 371, 13, 359, 5390, 345, 13603, 1053, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382, 2238, 611, 881, 13, 5390, 9830, 5584, 1264, 345, 28769, 2238, 548, 345, 6518]\n",
      "inputs:\n",
      "<s>  [INST] You are tasked with extracting relevant information or identification from the following key-value pairs. Given a piece of text, chapterize and generate a JSON format as output. Follow the instructions below to help you in generating the output:\n",
      "\n",
      "1. **Understand the Key-Value Structure:**\n",
      "   - A key-value pair consists of:\n",
      "      - `'summary'`: A top-level overview or description of the chunk. Must not be empty (i.e., required).\n",
      "      - `'headers'`: A list of headers where a header is a line of text that introduces a new section or chapter in a document. It is typically formatted distinctly from the main body text to stand out, often being bolder, in a larger font, or differently styled. Headers are concise, summarizing the content that follows, and they guide the reader through the document's structure. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'subheaders'`: A list of subheaders where subheaders are similar to headers but usually introduce subsections within a larger section. They are often formatted to be slightly less prominent than main headers, but still distinct from the body text. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'named_entities'`: A list of dictionary of named entities identified in the text. Must not be empty (i.e., required).\n",
      "      - `'keypoints'`: A list of the most important elements or essential information conveyed in the given text chunk. Must not be empty (i.e., required).\n",
      "      - `'tonality'`: The overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Must not be empty (i.e., required).\n",
      "\n",
      "2. **Extract Relevant Information:**\n",
      "   - Focus on extracting the most relevant information or identification from each chunk. This could include key themes, essential concepts, or significant details.\n",
      "\n",
      "3. **Perform Named Entity Recognition:**\n",
      "   - Extract named entities from a given text and organize the results, providing the option for individual descriptions for each entity. \n",
      "   - As part of the information extraction, identify and categorize any named entities, including but not limited to PERSON, ORGANIZATION, LOCATION, etc., within the text.\n",
      "   - The system should present the extracted entities in a structured format, allowing for detailed descriptions or additional information for each identified entity.  \n",
      "   - Follow exactly the example below for the named entities:\n",
      "\n",
      "      named_entities = [\n",
      "         {\n",
      "            \"<Tag_1>\": [\n",
      "                  {\"entity\": \"<Entity_1>\", \"description\": \"<Description_1a>\"},\n",
      "                  {\"entity\": \"<Entity_2>\", \"description\": \"<Description_1b>\"},\n",
      "                  # Add more entities with individual descriptions as needed\n",
      "            ]\n",
      "         },\n",
      "         {\n",
      "            \"<Tag_2>\": [\n",
      "                  {\"entity\": \"<Entity_3>\", \"description\": \"<Description_2>\"},\n",
      "                  {\"entity\": \"<Entity_4>\", \"description\": \"<Description_2>\"},\n",
      "                  # Add more entities with a common description as needed\n",
      "            ]\n",
      "         },\n",
      "         # Add more entries as needed\n",
      "      ]\n",
      "   \n",
      "   <Tag_1> and <Tag_2> represent different types of named entities.\n",
      "   Each tag contains a list of dictionaries, where each dictionary represents an entity and its associated description.\n",
      "   Each entity can have an individual description.\n",
      "\n",
      "\n",
      "4. **Determine the Tonality:**\n",
      "   - Assess the overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Include this assessment under the key `'tonality'`.\n",
      "\n",
      "5. **Highlight the Keypoints:**\n",
      "   - Identify and list the most important elements or essential information conveyed in the given chunk under the key `'keypoints'`.\n",
      "\n",
      "6. **Header Detection:**\n",
      "   - Analyze the text for characteristics typical to headers of a document:\n",
      "      - Capitalization: Look for any title case or all uppercase.\n",
      "      - Position: Consider if any text is at the beginning of a section or stands alone.\n",
      "      - Content: Assess if the text is concise and focused.\n",
      "      - Numbering/Symbols: Note any numbering or special symbols.\n",
      "      - Contextual Relevance: Evaluate if any text introduces a new topic or section.\n",
      "\n",
      "7. **Subheader Detection:**\n",
      "   - Analyze the text for characteristics typical of subheaders of a document:\n",
      "      - Capitalization: Subheaders often follow title case, but they might not always be in all uppercase, unlike some main headers.\n",
      "      - Position: Check if the text appears within a section, usually following a main header. Subheaders are often used to introduce subtopics within a larger section.\n",
      "      - Content: Determine if the text is more specific than a main header but still concise. It should elaborate on the content introduced by the main header.\n",
      "      - Numbering/Symbols: Look for secondary or nested numbering or bullet points (e.g., \"1.1 Subsection\", \"• Subpoint\").\n",
      "      - Contextual Relevance: Evaluate if the text refines or narrows down the topic introduced by the main header, rather than introducing a completely new topic.\n",
      "\n",
      "8. **Generate Tags**\n",
      "   - Please generate tags based on the given relevant information and strictly refrain from generating empty tags or empty array.\n",
      "\n",
      "9. **Provide Clear Details:**\n",
      "   - Clearly present the information extracted from the piece of text. Ensure that the details are concise and informative.\n",
      "\n",
      "10. **Strict JSON Response:**\n",
      "   - Strictly format your response as a JSON object. Each key-value pair should adhere to the specified structure.\n",
      "\n",
      "         Example Key-Value Pair:\n",
      "            {\n",
      "               \"summary\": \"It explores potential applications and future directions of research, including constructing dynamic knowledge graphs for specialized verticals, enhancing KGTransformer capabilities using self-supervised learning, combining language models with knowledge graphs, comparing graph learning techniques with traditional time-series models, and architectural improvements to KGTransformer.\",\n",
      "               \"headers\": [\"Introduction\", \"Methodology\", \"Discussion\"],\n",
      "               \"subheaders\": [\"Introduction to the Study\", \"Background and Context\", \"Research Methodology\", \"Implications of Results\", \"Future Research Directions\"],\n",
      "               \"named_entities\": \n",
      "                  [\n",
      "                     {\n",
      "                        \"Persons {\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with Hana.\"},\n",
      "        {\"entity\": \"Hana\", \"description\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382, 2238, 611, 881, 13, 5390, 9830, 5584, 1264, 345, 28769, 2238, 548, 345, 6518]\n",
      "labels:\n",
      "{\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with Hana.\"},\n",
      "        {\"entity\": \"Hana\", \"description\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 00:50:51,698 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 00:50:51,699 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 00:50:51 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 00:50:51 - INFO - llmtuner.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 00:50:51,717 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 00:50:51,717 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:50:51,718 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 00:50:54,376 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 00:50:54,376 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 00:50:54,682 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:50:54,682 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 00:50:54 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "05/17/2024 00:50:54 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 00:50:54 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 00:50:54 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:626] 2024-05-17 00:50:54,958 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2048] 2024-05-17 00:50:55,072 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-05-17 00:50:55,072 >>   Num examples = 743\n",
      "[INFO|trainer.py:2050] 2024-05-17 00:50:55,072 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2051] 2024-05-17 00:50:55,073 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2054] 2024-05-17 00:50:55,073 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2055] 2024-05-17 00:50:55,073 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2056] 2024-05-17 00:50:55,073 >>   Total optimization steps = 11\n",
      "[INFO|trainer.py:2057] 2024-05-17 00:50:55,074 >>   Number of trainable parameters = 3,407,872\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/finetune_forge/src/llmtuner/cli.py\", line 49, in main\n",
      "    run_exp()\n",
      "  File \"/opt/finetune_forge/src/llmtuner/train/tuner.py\", line 33, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/opt/finetune_forge/src/llmtuner/train/sft/workflow.py\", line 73, in run_sft\n",
      "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/transformers/trainer.py\", line 1859, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/transformers/trainer.py\", line 2203, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/transformers/trainer.py\", line 3147, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2125, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/_tensor.py\", line 525, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/autograd/function.py\", line 301, in apply\n",
      "    return user_fn(self, *args)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 320, in backward\n",
      "    torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.21 GiB. GPU \n",
      "  0%|                                                    | 0/11 [00:04<?, ?it/s]\n",
      "05/17/2024 00:51:35 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "05/17/2024 00:51:35 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:51:35,474 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:51:35,474 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:51:35,474 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:51:35,475 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 00:51:35,475 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 00:51:35 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "05/17/2024 00:51:35 - INFO - llmtuner.data.loader - Loading dataset MediaMeter/Chapterization...\n",
      "Converting format of dataset: 100%|██| 743/743 [00:00<00:00, 8414.79 examples/s]\n",
      "Running tokenizer on dataset: 100%|███| 743/743 [00:05<00:00, 126.89 examples/s]\n",
      "input_ids:\n",
      "[1, 28705, 733, 16289, 28793, 995, 460, 3638, 286, 395, 9131, 288, 8598, 1871, 442, 19451, 477, 272, 2296, 1945, 28733, 1431, 12690, 28723, 12628, 264, 5511, 302, 2245, 28725, 10661, 653, 304, 8270, 264, 9292, 5032, 390, 3825, 28723, 9822, 272, 11382, 3624, 298, 1316, 368, 297, 20365, 272, 3825, 28747, 13, 13, 28740, 28723, 619, 16778, 1793, 272, 7388, 28733, 1462, 3838, 8187, 4049, 13, 259, 387, 330, 1945, 28733, 1431, 5964, 12335, 302, 28747, 13, 355, 387, 1552, 28742, 3499, 28742, 28832, 28747, 330, 1830, 28733, 4404, 23094, 442, 5436, 302, 272, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 9899, 28742, 28832, 28747, 330, 1274, 302, 12489, 970, 264, 7735, 349, 264, 1407, 302, 2245, 369, 4180, 1377, 264, 633, 4211, 442, 10661, 297, 264, 3248, 28723, 661, 349, 9566, 1221, 11985, 9494, 346, 477, 272, 2191, 2187, 2245, 298, 1876, 575, 28725, 2608, 1250, 287, 3702, 28725, 297, 264, 6084, 6205, 28725, 442, 16841, 23138, 28723, 9655, 404, 460, 3078, 864, 28725, 18062, 3864, 272, 3036, 369, 6104, 28725, 304, 590, 8327, 272, 8847, 1059, 272, 3248, 28742, 28713, 4693, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1666, 9899, 28742, 28832, 28747, 330, 1274, 302, 1083, 9899, 970, 1083, 9899, 460, 3684, 298, 12489, 562, 4312, 13097, 1083, 24415, 2373, 264, 6084, 4211, 28723, 1306, 460, 2608, 1221, 11985, 298, 347, 7191, 2108, 15574, 821, 2191, 12489, 28725, 562, 1309, 9494, 477, 272, 2187, 2245, 28723, 3838, 848, 346, 28725, 456, 349, 264, 1274, 302, 11272, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 18561, 28730, 26644, 28742, 28832, 28747, 330, 1274, 302, 19443, 302, 5160, 19810, 10248, 297, 272, 2245, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 405, 1416, 804, 28713, 28742, 28832, 28747, 330, 1274, 302, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 2245, 11077, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 355, 387, 1552, 28742, 1158, 2045, 28742, 28832, 28747, 415, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 14242, 459, 347, 4606, 325, 28710, 28723, 28706, 2063, 3030, 609, 13, 13, 28750, 28723, 619, 24958, 1298, 7122, 9148, 4049, 13, 259, 387, 24408, 356, 9131, 288, 272, 1080, 8598, 1871, 442, 19451, 477, 1430, 11077, 28723, 851, 829, 3024, 1945, 18978, 28725, 7974, 16582, 28725, 442, 5864, 4162, 28723, 13, 13, 28770, 28723, 619, 4712, 674, 418, 3000, 20465, 3523, 3159, 685, 4049, 13, 259, 387, 1529, 2107, 5160, 19810, 477, 264, 2078, 2245, 304, 25425, 272, 2903, 28725, 7501, 272, 3551, 354, 3235, 25308, 354, 1430, 9040, 28723, 28705, 13, 259, 387, 1136, 744, 302, 272, 1871, 9237, 1774, 28725, 9051, 304, 20577, 653, 707, 5160, 19810, 28725, 2490, 562, 459, 6516, 298, 22015, 4780, 28725, 3994, 28777, 1251, 12889, 4866, 28725, 6807, 28743, 4866, 28725, 4345, 2063, 2373, 272, 2245, 28723, 13, 259, 387, 415, 1587, 1023, 2169, 272, 25081, 19810, 297, 264, 28429, 5032, 28725, 9836, 354, 10537, 25308, 442, 4870, 1871, 354, 1430, 10248, 9040, 28723, 259, 13, 259, 387, 9822, 4668, 272, 2757, 3624, 354, 272, 5160, 19810, 28747, 13, 13, 355, 5160, 28730, 26644, 327, 733, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28740, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28740, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28708, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28750, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28740, 28726, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 3235, 25308, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 371, 13, 17422, 6647, 4450, 28730, 28750, 28767, 1264, 733, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28770, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 9830, 5584, 1264, 6647, 5075, 28730, 28781, 20018, 345, 6518, 1264, 6647, 7301, 28730, 28750, 28767, 7706, 13, 359, 28705, 422, 3301, 680, 19810, 395, 264, 3298, 5436, 390, 3236, 13, 17422, 4709, 13, 273, 1630, 13, 273, 422, 3301, 680, 11507, 390, 3236, 13, 355, 4709, 13, 2287, 13, 259, 523, 4450, 28730, 28740, 28767, 304, 523, 4450, 28730, 28750, 28767, 2904, 1581, 4514, 302, 5160, 19810, 28723, 13, 259, 7066, 5846, 5876, 264, 1274, 302, 281, 3033, 4838, 28725, 970, 1430, 19443, 10651, 396, 9040, 304, 871, 5363, 5436, 28723, 13, 259, 7066, 9040, 541, 506, 396, 3235, 5436, 28723, 13, 13, 13, 28781, 28723, 619, 5155, 21824, 272, 17104, 2045, 4049, 13, 259, 387, 3348, 409, 272, 7544, 10294, 442, 21790, 302, 272, 2245, 11077, 28725, 1259, 390, 5278, 28725, 7087, 28725, 14214, 28725, 442, 9430, 28723, 560, 12856, 456, 15081, 916, 272, 1945, 1552, 28742, 1158, 2045, 28742, 9429, 13, 13, 28782, 28723, 619, 12822, 3646, 272, 4702, 1416, 804, 28713, 4049, 13, 259, 387, 15220, 1575, 304, 1274, 272, 1080, 2278, 5176, 442, 7974, 1871, 18887, 286, 297, 272, 2078, 11077, 916, 272, 1945, 1552, 28742, 405, 1416, 804, 28713, 28742, 9429, 13, 13, 28784, 28723, 619, 5404, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 298, 12489, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 6412, 354, 707, 3941, 1222, 442, 544, 7280, 2210, 28723, 13, 355, 387, 17361, 28747, 11772, 513, 707, 2245, 349, 438, 272, 5398, 302, 264, 4211, 442, 10969, 4411, 28723, 13, 355, 387, 13158, 28747, 3348, 409, 513, 272, 2245, 349, 3078, 864, 304, 9045, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6096, 707, 1474, 288, 442, 2841, 16212, 28723, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 707, 2245, 4180, 1377, 264, 633, 9067, 442, 4211, 28723, 13, 13, 28787, 28723, 619, 3540, 4983, 384, 22820, 4049, 13, 259, 387, 1094, 8910, 1374, 272, 2245, 354, 15559, 10842, 302, 1083, 9899, 302, 264, 3248, 28747, 13, 355, 387, 17053, 1837, 28747, 5078, 9899, 2608, 1372, 3941, 1222, 28725, 562, 590, 1659, 459, 1743, 347, 297, 544, 7280, 2210, 28725, 15343, 741, 2191, 12489, 28723, 13, 355, 387, 17361, 28747, 4914, 513, 272, 2245, 8045, 2373, 264, 4211, 28725, 4312, 2296, 264, 2191, 7735, 28723, 5078, 9899, 460, 2608, 1307, 298, 13097, 1083, 3746, 1063, 2373, 264, 6084, 4211, 28723, 13, 355, 387, 13158, 28747, 5158, 21824, 513, 272, 2245, 349, 680, 2948, 821, 264, 2191, 7735, 562, 1309, 3078, 864, 28723, 661, 1023, 24340, 356, 272, 3036, 8160, 486, 272, 2191, 7735, 28723, 13, 355, 387, 8978, 288, 28748, 9996, 28713, 28747, 6412, 354, 13461, 442, 23793, 1474, 288, 442, 17144, 3569, 325, 28706, 28723, 28721, 2063, 345, 28740, 28723, 28740, 5078, 3007, 548, 345, 28899, 5078, 2275, 4145, 13, 355, 387, 14268, 840, 1298, 2303, 617, 28747, 24223, 11931, 513, 272, 2245, 1792, 1303, 442, 8484, 28713, 1060, 272, 9067, 8160, 486, 272, 2191, 7735, 28725, 3210, 821, 25618, 264, 4716, 633, 9067, 28723, 13, 13, 28783, 28723, 619, 23342, 320, 1787, 348, 13, 259, 387, 5919, 8270, 12944, 2818, 356, 272, 2078, 8598, 1871, 304, 19470, 17154, 426, 477, 20365, 4606, 12944, 442, 4606, 2293, 28723, 13, 13, 28774, 28723, 619, 18325, 547, 16676, 16329, 4049, 13, 259, 387, 25421, 2169, 272, 1871, 25081, 477, 272, 5511, 302, 2245, 28723, 25217, 369, 272, 4162, 460, 3078, 864, 304, 5227, 1197, 28723, 13, 13, 28740, 28734, 28723, 619, 3490, 848, 9292, 12107, 4049, 13, 259, 387, 3838, 848, 346, 5032, 574, 2899, 390, 264, 9292, 1928, 28723, 7066, 1945, 28733, 1431, 5964, 1023, 616, 7750, 298, 272, 6140, 4693, 28723, 13, 13, 273, 16693, 7388, 28733, 1462, 367, 992, 28747, 13, 17422, 371, 13, 1417, 345, 3499, 1264, 345, 1313, 1949, 3626, 4628, 8429, 304, 3437, 14278, 302, 3332, 28725, 2490, 5122, 288, 10616, 4788, 23411, 354, 23427, 13519, 28713, 28725, 8050, 7161, 524, 28777, 10143, 263, 16585, 1413, 1008, 28733, 8554, 23313, 5168, 28725, 27698, 3842, 4994, 395, 4788, 23411, 28725, 21400, 5246, 5168, 9804, 395, 7062, 727, 28733, 17384, 4994, 28725, 304, 7821, 1890, 19664, 298, 524, 28777, 10143, 263, 9191, 13, 1417, 345, 9899, 1264, 7367, 27230, 548, 345, 3942, 2161, 548, 345, 3278, 18637, 8883, 13, 1417, 345, 1666, 9899, 1264, 7367, 27230, 298, 272, 18463, 548, 345, 11278, 304, 14268, 548, 345, 1146, 3625, 10011, 2161, 548, 345, 2012, 12909, 302, 17412, 548, 345, 17147, 7982, 19370, 1308, 8883, 13, 1417, 345, 18561, 28730, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382, 2238, 611, 881, 13, 5390, 9830, 5584, 1264, 345, 28769, 2238]\n",
      "inputs:\n",
      "<s>  [INST] You are tasked with extracting relevant information or identification from the following key-value pairs. Given a piece of text, chapterize and generate a JSON format as output. Follow the instructions below to help you in generating the output:\n",
      "\n",
      "1. **Understand the Key-Value Structure:**\n",
      "   - A key-value pair consists of:\n",
      "      - `'summary'`: A top-level overview or description of the chunk. Must not be empty (i.e., required).\n",
      "      - `'headers'`: A list of headers where a header is a line of text that introduces a new section or chapter in a document. It is typically formatted distinctly from the main body text to stand out, often being bolder, in a larger font, or differently styled. Headers are concise, summarizing the content that follows, and they guide the reader through the document's structure. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'subheaders'`: A list of subheaders where subheaders are similar to headers but usually introduce subsections within a larger section. They are often formatted to be slightly less prominent than main headers, but still distinct from the body text. Strictly, this is a list of strings. Must not be empty (i.e., required).\n",
      "      - `'named_entities'`: A list of dictionary of named entities identified in the text. Must not be empty (i.e., required).\n",
      "      - `'keypoints'`: A list of the most important elements or essential information conveyed in the given text chunk. Must not be empty (i.e., required).\n",
      "      - `'tonality'`: The overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Must not be empty (i.e., required).\n",
      "\n",
      "2. **Extract Relevant Information:**\n",
      "   - Focus on extracting the most relevant information or identification from each chunk. This could include key themes, essential concepts, or significant details.\n",
      "\n",
      "3. **Perform Named Entity Recognition:**\n",
      "   - Extract named entities from a given text and organize the results, providing the option for individual descriptions for each entity. \n",
      "   - As part of the information extraction, identify and categorize any named entities, including but not limited to PERSON, ORGANIZATION, LOCATION, etc., within the text.\n",
      "   - The system should present the extracted entities in a structured format, allowing for detailed descriptions or additional information for each identified entity.  \n",
      "   - Follow exactly the example below for the named entities:\n",
      "\n",
      "      named_entities = [\n",
      "         {\n",
      "            \"<Tag_1>\": [\n",
      "                  {\"entity\": \"<Entity_1>\", \"description\": \"<Description_1a>\"},\n",
      "                  {\"entity\": \"<Entity_2>\", \"description\": \"<Description_1b>\"},\n",
      "                  # Add more entities with individual descriptions as needed\n",
      "            ]\n",
      "         },\n",
      "         {\n",
      "            \"<Tag_2>\": [\n",
      "                  {\"entity\": \"<Entity_3>\", \"description\": \"<Description_2>\"},\n",
      "                  {\"entity\": \"<Entity_4>\", \"description\": \"<Description_2>\"},\n",
      "                  # Add more entities with a common description as needed\n",
      "            ]\n",
      "         },\n",
      "         # Add more entries as needed\n",
      "      ]\n",
      "   \n",
      "   <Tag_1> and <Tag_2> represent different types of named entities.\n",
      "   Each tag contains a list of dictionaries, where each dictionary represents an entity and its associated description.\n",
      "   Each entity can have an individual description.\n",
      "\n",
      "\n",
      "4. **Determine the Tonality:**\n",
      "   - Assess the overall tone or sentiment of the text chunk, such as positive, negative, neutral, or mixed. Include this assessment under the key `'tonality'`.\n",
      "\n",
      "5. **Highlight the Keypoints:**\n",
      "   - Identify and list the most important elements or essential information conveyed in the given chunk under the key `'keypoints'`.\n",
      "\n",
      "6. **Header Detection:**\n",
      "   - Analyze the text for characteristics typical to headers of a document:\n",
      "      - Capitalization: Look for any title case or all uppercase.\n",
      "      - Position: Consider if any text is at the beginning of a section or stands alone.\n",
      "      - Content: Assess if the text is concise and focused.\n",
      "      - Numbering/Symbols: Note any numbering or special symbols.\n",
      "      - Contextual Relevance: Evaluate if any text introduces a new topic or section.\n",
      "\n",
      "7. **Subheader Detection:**\n",
      "   - Analyze the text for characteristics typical of subheaders of a document:\n",
      "      - Capitalization: Subheaders often follow title case, but they might not always be in all uppercase, unlike some main headers.\n",
      "      - Position: Check if the text appears within a section, usually following a main header. Subheaders are often used to introduce subtopics within a larger section.\n",
      "      - Content: Determine if the text is more specific than a main header but still concise. It should elaborate on the content introduced by the main header.\n",
      "      - Numbering/Symbols: Look for secondary or nested numbering or bullet points (e.g., \"1.1 Subsection\", \"• Subpoint\").\n",
      "      - Contextual Relevance: Evaluate if the text refines or narrows down the topic introduced by the main header, rather than introducing a completely new topic.\n",
      "\n",
      "8. **Generate Tags**\n",
      "   - Please generate tags based on the given relevant information and strictly refrain from generating empty tags or empty array.\n",
      "\n",
      "9. **Provide Clear Details:**\n",
      "   - Clearly present the information extracted from the piece of text. Ensure that the details are concise and informative.\n",
      "\n",
      "10. **Strict JSON Response:**\n",
      "   - Strictly format your response as a JSON object. Each key-value pair should adhere to the specified structure.\n",
      "\n",
      "         Example Key-Value Pair:\n",
      "            {\n",
      "               \"summary\": \"It explores potential applications and future directions of research, including constructing dynamic knowledge graphs for specialized verticals, enhancing KGTransformer capabilities using self-supervised learning, combining language models with knowledge graphs, comparing graph learning techniques with traditional time-series models, and architectural improvements to KGTransformer.\",\n",
      "               \"headers\": [\"Introduction\", \"Methodology\", \"Discussion\"],\n",
      "               \"subheaders\": [\"Introduction to the Study\", \"Background and Context\", \"Research Methodology\", \"Implications of Results\", \"Future Research Directions\"],\n",
      "               \"named_ {\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with Hana.\"},\n",
      "        {\"entity\": \"Hana\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 371, 13, 28705, 345, 3499, 1264, 345, 1014, 12280, 9819, 1002, 272, 4630, 304, 26358, 11226, 3758, 1444, 6849, 20785, 304, 382, 2238, 28725, 12144, 288, 652, 22772, 9235, 28725, 5593, 15852, 354, 6965, 7556, 734, 28725, 304, 272, 1951, 840, 744, 288, 302, 4342, 390, 382, 2238, 27297, 17253, 304, 264, 633, 1411, 297, 19887, 28723, 415, 2838, 4286, 1238, 18978, 302, 9539, 1007, 1879, 449, 4703, 28725, 12573, 5174, 28725, 304, 272, 24809, 302, 1008, 28733, 11360, 5298, 5277, 9293, 9191, 13, 28705, 345, 9899, 1264, 7367, 27230, 548, 345, 28735, 328, 20785, 304, 382, 2238, 28742, 28713, 5855, 18798, 548, 345, 28769, 2238, 28742, 28713, 3995, 444, 482, 548, 345, 28735, 328, 20785, 28742, 28713, 6360, 1784, 548, 345, 1925, 14324, 297, 19887, 8883, 13, 28705, 345, 1666, 9899, 1264, 7367, 7489, 1618, 12520, 548, 345, 11491, 14497, 4335, 5286, 548, 345, 2571, 3314, 9870, 788, 548, 345, 28769, 2238, 28742, 28713, 330, 14079, 697, 548, 345, 4917, 288, 394, 748, 548, 345, 22348, 1298, 14324, 548, 345, 28769, 2238, 28742, 28713, 5372, 297, 19887, 8883, 13, 28705, 345, 18561, 28730, 26644, 1264, 733, 13, 2287, 371, 13, 355, 345, 13603, 1053, 1264, 733, 13, 5390, 9830, 5584, 1264, 345, 28735, 328, 20785, 548, 345, 6518, 1264, 345, 28741, 2518, 676, 5290, 297, 264, 4630, 3758, 395, 382, 2238, 611, 881, 13, 5390, 9830, 5584, 1264, 345, 28769, 2238]\n",
      "labels:\n",
      "{\n",
      "  \"summary\": \"The passage narrates the complex and emotionally charged relationship between Solomon and Hana, highlighting their intimate moments, financial transactions for sexual favors, and the eventual parting of ways as Hana seeks independence and a new life in Tokyo. The story captures themes of youthful naivety, exploitation, and the pursuit of self-worth beyond physical appearance.\",\n",
      "  \"headers\": [\"Introduction\", \"Solomon and Hana's Relationship\", \"Hana's Departure\", \"Solomon's Reflection\", \"Reunion in Tokyo\"],\n",
      "  \"subheaders\": [\"First Encounter\", \"Financial Transactions\", \"Intimate Moments\", \"Hana's Aspirations\", \"Parting Ways\", \"Unexpected Reunion\", \"Hana's Life in Tokyo\"],\n",
      "  \"named_entities\": [\n",
      "    {\n",
      "      \"Persons\": [\n",
      "        {\"entity\": \"Solomon\", \"description\": \"A young man involved in a complex relationship with Hana.\"},\n",
      "        {\"entity\": \"Hana\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 00:51:48,334 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 00:51:48,335 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 00:51:48 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 00:51:48 - INFO - llmtuner.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 00:51:48,354 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 00:51:48,355 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:51:48,355 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.38it/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 00:51:51,010 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 00:51:51,010 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 00:51:51,310 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 00:51:51,310 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 00:51:51 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "05/17/2024 00:51:51 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 00:51:51 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 00:51:51 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:626] 2024-05-17 00:51:51,606 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2048] 2024-05-17 00:51:51,724 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-05-17 00:51:51,724 >>   Num examples = 743\n",
      "[INFO|trainer.py:2050] 2024-05-17 00:51:51,724 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2051] 2024-05-17 00:51:51,724 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2054] 2024-05-17 00:51:51,724 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2055] 2024-05-17 00:51:51,724 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2056] 2024-05-17 00:51:51,724 >>   Total optimization steps = 11\n",
      "[INFO|trainer.py:2057] 2024-05-17 00:51:51,726 >>   Number of trainable parameters = 3,407,872\n",
      " 45%|████████████████████                        | 5/11 [03:20<04:00, 40.05s/it]05/17/2024 00:55:12 - INFO - llmtuner.extras.callbacks - {'loss': 1.6531, 'learning_rate': 1.1423e-04, 'epoch': 0.43}\n",
      "{'loss': 1.6531, 'grad_norm': 0.8333468437194824, 'learning_rate': 0.00011423148382732853, 'epoch': 0.43}\n",
      " 91%|███████████████████████████████████████    | 10/11 [06:40<00:40, 40.04s/it]05/17/2024 00:58:32 - INFO - llmtuner.extras.callbacks - {'loss': 1.4784, 'learning_rate': 4.0507e-06, 'epoch': 0.85}\n",
      "{'loss': 1.4784, 'grad_norm': 0.744696319103241, 'learning_rate': 4.050702638550275e-06, 'epoch': 0.85}\n",
      "100%|███████████████████████████████████████████| 11/11 [07:20<00:00, 40.04s/it][INFO|trainer.py:2316] 2024-05-17 00:59:12,354 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 440.6281, 'train_samples_per_second': 1.686, 'train_steps_per_second': 0.025, 'train_loss': 1.5508125912059436, 'epoch': 0.94}\n",
      "100%|███████████████████████████████████████████| 11/11 [07:20<00:00, 40.06s/it]\n",
      "[INFO|trainer.py:3305] 2024-05-17 00:59:12,355 >> Saving model checkpoint to saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 00:59:13,033 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 00:59:13,034 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-05-17 00:59:13,051 >> tokenizer config file saved in saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-05-17 00:59:13,051 >> Special tokens file saved in saves/Mistral-7B-v0.2-Chat/lora/Chapterization_Mistral-7B-v0.2-Chat_0.1.1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9362\n",
      "  total_flos               = 47016612GF\n",
      "  train_loss               =     1.5508\n",
      "  train_runtime            = 0:07:20.62\n",
      "  train_samples_per_second =      1.686\n",
      "  train_steps_per_second   =      0.025\n",
      "[INFO|modelcard.py:450] 2024-05-17 00:59:13,067 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "05/17/2024 01:02:58 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "05/17/2024 01:02:58 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:02:59,317 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:02:59,317 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:02:59,317 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:02:59,317 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:02:59,317 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:02:59 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "05/17/2024 01:02:59 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
      "Converting format of dataset: 100%|█| 2415/2415 [00:00<00:00, 64127.81 examples/\n",
      "Running tokenizer on dataset: 100%|█| 2415/2415 [00:00<00:00, 4757.44 examples/s\n",
      "input_ids:\n",
      "[1, 28705, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 733, 28748, 16289, 28793, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "inputs:\n",
      "<s>  [INST] translate this sentence in docker command\n",
      "Give me a list of containers that have the Ubuntu image as their ancestor. [/INST] docker ps --filter 'ancestor=ubuntu'</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "labels:\n",
      "docker ps --filter 'ancestor=ubuntu'</s>\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:03:04,791 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:03:04,792 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:03:04 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:03:04 - INFO - llmtuner.model.utils.quantization - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:03:04,810 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:03:04,811 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:03:04,811 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.33it/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:03:07,576 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:03:07,576 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:03:07,838 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:03:07,838 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:03:08 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "05/17/2024 01:03:08 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:03:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:03:08 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:626] 2024-05-17 01:03:08,126 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2048] 2024-05-17 01:03:08,241 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-05-17 01:03:08,241 >>   Num examples = 2,415\n",
      "[INFO|trainer.py:2050] 2024-05-17 01:03:08,241 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2051] 2024-05-17 01:03:08,241 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2054] 2024-05-17 01:03:08,242 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2055] 2024-05-17 01:03:08,242 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2056] 2024-05-17 01:03:08,242 >>   Total optimization steps = 37\n",
      "[INFO|trainer.py:2057] 2024-05-17 01:03:08,243 >>   Number of trainable parameters = 3,407,872\n",
      " 14%|█████▉                                      | 5/37 [00:10<01:04,  2.01s/it]05/17/2024 01:03:18 - INFO - llmtuner.extras.callbacks - {'loss': 1.7488, 'learning_rate': 1.9112e-04, 'epoch': 0.13}\n",
      "{'loss': 1.7488, 'grad_norm': 5.354613304138184, 'learning_rate': 0.0001911228490388136, 'epoch': 0.13}\n",
      " 27%|███████████▌                               | 10/37 [00:20<00:54,  2.03s/it]05/17/2024 01:03:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.3870, 'learning_rate': 1.6607e-04, 'epoch': 0.26}\n",
      "{'loss': 0.387, 'grad_norm': 2.1430466175079346, 'learning_rate': 0.00016606747233900815, 'epoch': 0.26}\n",
      " 41%|█████████████████▍                         | 15/37 [00:30<00:44,  2.00s/it]05/17/2024 01:03:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.1989, 'learning_rate': 1.2928e-04, 'epoch': 0.40}\n",
      "{'loss': 0.1989, 'grad_norm': 1.2286360263824463, 'learning_rate': 0.00012928227712765504, 'epoch': 0.4}\n",
      " 54%|███████████████████████▏                   | 20/37 [00:40<00:33,  1.96s/it]05/17/2024 01:03:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.1617, 'learning_rate': 8.7298e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1617, 'grad_norm': 1.808967113494873, 'learning_rate': 8.729821802531212e-05, 'epoch': 0.53}\n",
      " 68%|█████████████████████████████              | 25/37 [00:50<00:23,  1.97s/it]05/17/2024 01:03:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1000, 'learning_rate': 4.7569e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1, 'grad_norm': 1.2328404188156128, 'learning_rate': 4.756927164427685e-05, 'epoch': 0.66}\n",
      " 81%|██████████████████████████████████▊        | 30/37 [01:00<00:13,  1.97s/it]05/17/2024 01:04:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0975, 'learning_rate': 1.7149e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0975, 'grad_norm': 0.9569980502128601, 'learning_rate': 1.7149035075615794e-05, 'epoch': 0.79}\n",
      " 95%|████████████████████████████████████████▋  | 35/37 [01:10<00:04,  2.02s/it]05/17/2024 01:04:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0898, 'learning_rate': 1.4384e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0898, 'grad_norm': 0.6265817284584045, 'learning_rate': 1.4384089652291543e-06, 'epoch': 0.93}\n",
      "100%|███████████████████████████████████████████| 37/37 [01:14<00:00,  2.06s/it][INFO|trainer.py:2316] 2024-05-17 01:04:22,847 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 74.6037, 'train_samples_per_second': 32.371, 'train_steps_per_second': 0.496, 'train_loss': 0.3808064919871253, 'epoch': 0.98}\n",
      "100%|███████████████████████████████████████████| 37/37 [01:14<00:00,  2.02s/it]\n",
      "[INFO|trainer.py:3305] 2024-05-17 01:04:22,848 >> Saving model checkpoint to saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:04:23,393 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:04:23,393 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-05-17 01:04:23,412 >> tokenizer config file saved in saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-05-17 01:04:23,412 >> Special tokens file saved in saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9801\n",
      "  total_flos               =  7256026GF\n",
      "  train_loss               =     0.3808\n",
      "  train_runtime            = 0:01:14.60\n",
      "  train_samples_per_second =     32.371\n",
      "  train_steps_per_second   =      0.496\n",
      "[INFO|modelcard.py:450] 2024-05-17 01:04:23,429 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "05/17/2024 01:05:04 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "05/17/2024 01:05:04 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:05:05,803 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:05:05,804 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:05:05,804 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:05:05,804 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:05:05,804 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:05:05 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "05/17/2024 01:05:05 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
      "input_ids:\n",
      "[1, 28705, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 733, 28748, 16289, 28793, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "inputs:\n",
      "<s>  [INST] translate this sentence in docker command\n",
      "Give me a list of containers that have the Ubuntu image as their ancestor. [/INST] docker ps --filter 'ancestor=ubuntu'</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "labels:\n",
      "docker ps --filter 'ancestor=ubuntu'</s>\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:05:15,124 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:05:15,125 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:05:15 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:05:15 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:05:15,144 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:05:15,145 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:05:15,244 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.33s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:05:19,730 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:05:19,731 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:05:19,988 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:05:19,989 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:05:20 - INFO - llmtuner.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "05/17/2024 01:05:20 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:05:20 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:05:20 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:626] 2024-05-17 01:05:20,212 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2048] 2024-05-17 01:05:20,330 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-05-17 01:05:20,330 >>   Num examples = 2,415\n",
      "[INFO|trainer.py:2050] 2024-05-17 01:05:20,330 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2051] 2024-05-17 01:05:20,330 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2054] 2024-05-17 01:05:20,330 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2055] 2024-05-17 01:05:20,330 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2056] 2024-05-17 01:05:20,330 >>   Total optimization steps = 37\n",
      "[INFO|trainer.py:2057] 2024-05-17 01:05:20,332 >>   Number of trainable parameters = 3,407,872\n",
      "  0%|                                                    | 0/37 [00:00<?, ?it/s]/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█████▉                                      | 5/37 [00:22<02:23,  4.48s/it]05/17/2024 01:05:43 - INFO - llmtuner.extras.callbacks - {'loss': 1.8282, 'learning_rate': 1.9112e-04, 'epoch': 0.13}\n",
      "{'loss': 1.8282, 'grad_norm': 4.712010860443115, 'learning_rate': 0.0001911228490388136, 'epoch': 0.13}\n",
      " 27%|███████████▌                               | 10/37 [00:45<02:00,  4.46s/it]05/17/2024 01:06:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.3941, 'learning_rate': 1.6607e-04, 'epoch': 0.26}\n",
      "{'loss': 0.3941, 'grad_norm': 2.0239531993865967, 'learning_rate': 0.00016606747233900815, 'epoch': 0.26}\n",
      " 41%|█████████████████▍                         | 15/37 [01:07<01:36,  4.39s/it]05/17/2024 01:06:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.2029, 'learning_rate': 1.2928e-04, 'epoch': 0.40}\n",
      "{'loss': 0.2029, 'grad_norm': 1.1798151731491089, 'learning_rate': 0.00012928227712765504, 'epoch': 0.4}\n",
      " 54%|███████████████████████▏                   | 20/37 [01:28<01:14,  4.35s/it]05/17/2024 01:06:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1686, 'learning_rate': 8.7298e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1686, 'grad_norm': 1.8161628246307373, 'learning_rate': 8.729821802531212e-05, 'epoch': 0.53}\n",
      " 68%|█████████████████████████████              | 25/37 [01:50<00:52,  4.34s/it]05/17/2024 01:07:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.1049, 'learning_rate': 4.7569e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1049, 'grad_norm': 1.2765902280807495, 'learning_rate': 4.756927164427685e-05, 'epoch': 0.66}\n",
      " 81%|██████████████████████████████████▊        | 30/37 [02:12<00:30,  4.29s/it]05/17/2024 01:07:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.1008, 'learning_rate': 1.7149e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1008, 'grad_norm': 0.9165561199188232, 'learning_rate': 1.7149035075615794e-05, 'epoch': 0.79}\n",
      " 95%|████████████████████████████████████████▋  | 35/37 [02:34<00:08,  4.40s/it]05/17/2024 01:07:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0948, 'learning_rate': 1.4384e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0948, 'grad_norm': 0.6189349293708801, 'learning_rate': 1.4384089652291543e-06, 'epoch': 0.93}\n",
      "100%|███████████████████████████████████████████| 37/37 [02:43<00:00,  4.45s/it][INFO|trainer.py:2316] 2024-05-17 01:08:03,705 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 163.3731, 'train_samples_per_second': 14.782, 'train_steps_per_second': 0.226, 'train_loss': 0.3962263984454645, 'epoch': 0.98}\n",
      "100%|███████████████████████████████████████████| 37/37 [02:43<00:00,  4.42s/it]\n",
      "[INFO|trainer.py:3305] 2024-05-17 01:08:03,706 >> Saving model checkpoint to saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.1\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:08:04,267 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:08:04,268 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-05-17 01:08:04,288 >> tokenizer config file saved in saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-05-17 01:08:04,288 >> Special tokens file saved in saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9801\n",
      "  total_flos               =  7256026GF\n",
      "  train_loss               =     0.3962\n",
      "  train_runtime            = 0:02:43.37\n",
      "  train_samples_per_second =     14.782\n",
      "  train_steps_per_second   =      0.226\n",
      "[INFO|modelcard.py:450] 2024-05-17 01:08:04,310 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:09:34,765 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:09:34,765 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:09:34,765 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:09:34,765 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:09:34,765 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:09:34 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:09:35,672 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:09:35,672 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:09:35 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:09:35 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:09:35 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:09:35,691 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:09:35,692 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:09:35,693 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.38s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:09:40,317 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:09:40,317 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:09:41,518 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:09:41,518 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:09:41 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:09:41 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:09:41 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0\n",
      "05/17/2024 01:09:41 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "/opt/finetune_forge/.venv_llama_factory_vanilla/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:10:59,106 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:10:59,106 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:10:59,106 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:10:59,106 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:10:59,106 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:10:59 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:10:59,443 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:10:59,444 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:10:59 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:10:59 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:10:59 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:10:59,445 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:10:59,446 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:10:59,446 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.32s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:11:03,873 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:11:03,873 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:11:04,203 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:11:04,203 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:11:04 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:11:04 - INFO - llmtuner.model.adapter - Adapter is not found at evaluation, load the base model.\n",
      "05/17/2024 01:11:04 - INFO - llmtuner.model.loader - all params: 7241732096\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:12:52,994 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:12:52,994 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:12:52,994 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:12:52,994 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:12:52,994 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:12:53 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:12:53,888 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:12:53,888 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:12:53 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:12:53 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:12:53 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:12:53,890 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:12:53,890 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:12:53,891 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.36s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:12:58,423 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:12:58,423 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:12:59,243 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:12:59,243 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:12:59 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:12:59 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:12:59 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.1\n",
      "05/17/2024 01:12:59 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:14:05,488 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:14:05,488 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:14:05,488 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:14:05,488 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:14:05,488 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:14:05 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:14:05,794 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:14:05,794 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:14:05 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:14:05 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:14:05 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:14:05,796 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:14:05,796 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:14:05,797 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.29s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:14:10,112 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:14:10,112 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:14:10,371 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:14:10,372 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:14:10 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:14:10 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:14:10 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0\n",
      "05/17/2024 01:14:10 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:16:58,980 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:16:58,980 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:16:58,980 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:16:58,980 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:16:58,980 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:16:59 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:16:59,326 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:16:59,326 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:16:59 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:16:59 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:16:59 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:16:59,328 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:16:59,328 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:16:59,329 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.27s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:17:03,626 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:17:03,626 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:17:03,936 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:17:03,936 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:17:04 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:17:04 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:17:04 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.1\n",
      "05/17/2024 01:17:04 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:17:41,544 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:17:41,544 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:17:41,544 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:17:41,544 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:17:41,544 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:17:41 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:17:42,398 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:17:42,399 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:17:42 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:17:42 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:17:42 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:17:42,400 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:17:42,401 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:17:42,401 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.25s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:17:46,591 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:17:46,591 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:17:47,379 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:17:47,379 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:17:47 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:17:47 - INFO - llmtuner.model.adapter - Adapter is not found at evaluation, load the base model.\n",
      "05/17/2024 01:17:47 - INFO - llmtuner.model.loader - all params: 7241732096\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:19:34,323 >> loading file tokenizer.model from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:19:34,323 >> loading file tokenizer.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:19:34,323 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:19:34,323 >> loading file special_tokens_map.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-05-17 01:19:34,323 >> loading file tokenizer_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n",
      "05/17/2024 01:19:34 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "[INFO|configuration_utils.py:726] 2024-05-17 01:19:35,169 >> loading configuration file config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-17 01:19:35,170 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "05/17/2024 01:19:35 - WARNING - llmtuner.model.utils.attention - FlashAttention-2 is not installed.\n",
      "05/17/2024 01:19:35 - INFO - llmtuner.model.utils.quantization - Quantizing model to 8 bit.\n",
      "05/17/2024 01:19:35 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3429] 2024-05-17 01:19:35,171 >> loading weights file model.safetensors from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1494] 2024-05-17 01:19:35,172 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:19:35,172 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.38s/it]\n",
      "[INFO|modeling_utils.py:4170] 2024-05-17 01:19:39,730 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-05-17 01:19:39,730 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-05-17 01:19:40,011 >> loading configuration file generation_config.json from cache at /home/marx/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-05-17 01:19:40,011 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "05/17/2024 01:19:40 - INFO - llmtuner.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "05/17/2024 01:19:40 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "05/17/2024 01:19:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Mistral-7B-v0.2-Chat/lora/Dockerize_Mistral-7B-v0.2-Chat_0.1.0\n",
      "05/17/2024 01:19:40 - INFO - llmtuner.model.loader - all params: 7245139968\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 src/webui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_llama_factory_vanilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
